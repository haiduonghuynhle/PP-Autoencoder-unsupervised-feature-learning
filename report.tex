\documentclass[a4paper, 13pt]{extarticle}
\usepackage{a4wide,amssymb,epsfig,latexsym,multicol,array,hhline,fancyhdr}
\usepackage{matlab-prettifier}
\usepackage{amsmath}
\usepackage{lastpage}
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}
\usepackage{array}
\usepackage{tabularx, caption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{rotating}
\usepackage{graphics}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{epsfig}
\usepackage{tikz}
\usepackage{url}
\usepackage{xurl}
\usepackage{hyperref}
\usepackage{scrextend}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage[utf8]{inputenc} % Allow UTF-8 characters
\usepackage[T5]{fontenc}  
\usepackage[english]{babel}
\usepackage{mathptmx} % Font Times New Roman
\usepackage{spverbatim}
\usepackage{placeins}
\usepackage{subcaption} % For subfigures
\usetikzlibrary{arrows,snakes,backgrounds}
\hypersetup{urlcolor=blue,linkcolor=black,citecolor=black,colorlinks=true}

\newtheorem{theorem}{{\bf Theorem}}
\newtheorem{property}{{\bf Property}}
\newtheorem{proposition}{{\bf Proposition}}
\newtheorem{corollary}[proposition]{{\bf Corollary}}
\newtheorem{lemma}[proposition]{{\bf Lemma}}

\AtBeginDocument{\renewcommand*\contentsname{Table of Contents}}
\AtBeginDocument{\renewcommand*\refname{References}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\setlength{\headheight}{40pt}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\fancyhead[L]{
 \begin{tabular}{rl}
    \begin{picture}(25,15)(0,0)
    \put(0,-8){\includegraphics[width=9.8mm, height=9mm]{hcmus.png}}
   \end{picture}&
    \begin{tabular}{l}
        \textbf{\bf \ttfamily University of Science, Ho Chi Minh City}\\
        \textbf{\bf \ttfamily Faculty of Information Technology}\\
    \end{tabular}
 \end{tabular}
}
\fancyhead[R]{
    \begin{tabular}{l}
        \tiny \bf \\
        \tiny \bf
    \end{tabular}
}
\fancyfoot{} % clear all footer fields
\fancyfoot[L]{\scriptsize \ttfamily CUDA Autoencoder for CIFAR-10 Feature Learning}
\fancyfoot[R]{\scriptsize \ttfamily Page {\thepage}/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}

\renewcommand{\baselinestretch}{1.15}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}
\makeatletter
\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection .\@alph\c@subsubsubsection}
\newcommand\subsubsubsection{\@startsection{subsubsubsection}{4}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\newcommand*\l@subsubsubsection{\@dottedtocline{3}{10.0em}{4.1em}}
\newcommand*{\subsubsubsectionmark}[1]{}
\makeatother

\begin{document}

%==========================
%         TRANG BÌA
%==========================
\begin{titlepage}
\begin{center}
VIETNAM NATIONAL UNIVERSITY, HO CHI MINH CITY \\
UNIVERSITY OF SCIENCE \\
FACULTY OF INFORMATION TECHNOLOGY\\
\end{center}

\vspace{1cm}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=5cm]{hcmus.png}
\end{center}
\end{figure}

\vspace{0cm}

\begin{center}
\begin{tabular}{c}
\multicolumn{1}{c}{\textbf{{\Large Parallels Processing}}}\\
~~\\
\hline
\\
\textbf{{\Large Final Report }}\\\\
\textbf{{\Large Team Contribution}}\\\\
\hline
\end{tabular}
\end{center}

\vspace{1cm}

\begin{table}[h]
\begin{tabular}{rrl}
\hspace{3.3 cm} & Instructor: & Phạm Trọng Nghĩa\\
& & Lê Nhựt Nam\\

\end{tabular}
\end{table}

\noindent\hspace*{4cm}Group members:\par\vspace{0.5cm}
\begin{adjustbox}{center}
    \begin{tabular}{|c|c|c|}
        \hline
         \textbf{No.} & \textbf{Full name} & \textbf{Student ID} \\
         \hline
         1 & Huỳnh Lê Hải Dương & 22127081\\
         \hline
         2 & Nguyễn Triều Khoáng & 22127204\\
         \hline
    \end{tabular}
\end{adjustbox}


\vspace{2.5 cm}
\begin{center}
{\footnotesize HO CHI MINH CITY, 2025}
\end{center}
\end{titlepage}


%==========================
%        ABSTRACT
%==========================
\section*{Team planning}
\addcontentsline{toc}{section}{Abstract}


\subsection*{Work contribution}
\begin{center}
\begin{tabular}{|c|p{3cm}|p{2.3cm}|c|p{4.5cm}|p{1.5cm}|}
\hline
\textbf{No.} & \textbf{Task name} & \textbf{Student} & \textbf{Deadline} & \textbf{Details} & \textbf{Notes} \\ \hline
1 & Literature Review \& Project Setup & Hải Dương, Triều Khoáng & 01/12/2025 & Research autoencoder architecture, CUDA programming. Setup environment, download CIFAR-10 dataset. & Finished (100\%) \\ \hline
2 & Data Loader Implementation & Hải Dương, Triều Khoáng  & 01/12/2025 & Implement CIFAR-10 binary file loader, batch generator with shuffling, data normalization. & Finished (100\%)\\ \hline
3 & CPU Baseline Implementation & Hải Dương & 04/12/2025 & Implement CPU layers (Conv2D, MaxPool2D, ReLU, Upsampling), CPU autoencoder, training loop with backpropagation. & Finished (100\%)\\ \hline
4 & GPU Basic Implementation & Hải Dương & 07/12/2025 & Port layers to GPU, implement CUDA kernels for convolution, pooling, ReLU. GPU autoencoder with memory management. & Finished (100\%)\\ \hline
5 & GPU Optimization V1 (Shared Memory) & Triều Khoáng & 10/12/2025 & Implement shared memory tiling for convolution, reduce global memory accesses, optimize memory access patterns. & Finished (100\%)\\ \hline
6 & GPU Optimization V2 (Fusion + Streams) & Triều Khoáng & 10/12/2025 & Kernel fusion (Conv2D + ReLU + Bias), CUDA streams for computation/data transfer overlap, vectorized memory access. & Finished (100\%)\\ \hline
7 & Feature Extraction & Triều Khoáng & 13/12/2025 & Implement encoder feature extraction, save features to binary format, support both CPU and GPU paths. & Finished (100\%)\\ \hline
8 & SVM Training Pipeline & Triều Khoáng & 15/12/2025 & Integrate cuML SVM, load features and labels, train and evaluate SVM classifier, performance benchmarking. & Finished (100\%)\\ \hline
9 & Demo Interface & Hải Dương, Triều Khoáng & 16/12/2025 & Gradio web interface, load trained models (autoencoder + SVM), real-time image classification, visualization. & Finished (100\%)\\ \hline
\end{tabular}
\end{center}

\newpage

\begin{center}
\begin{tabular}{|c|p{3cm}|p{2.3cm}|c|p{4.5cm}|p{1.5cm}|}
\hline
\textbf{No.} & \textbf{Task name} & \textbf{Student} & \textbf{Deadline} & \textbf{Details} & \textbf{Notes} \\ \hline
10 & Performance Analysis \& Documentation & Hải Dương, Triều Khoáng & 17/12/2025 & Performance profiling and benchmarking, create visualization charts, write Jupyter notebook report, code documentation. & Finished (100\%)\\ \hline
11 & Video demo & Hải Dương, Triều Khoáng & 18/12/2025 & Record and edit video demonstration of the complete system, showcase all features and performance results. & Finished (100\%)\\ \hline
\end{tabular}
\end{center}

\subsection*{Timeline}
\begin{center}
\begin{tabular}{|c|p{3.5cm}|p{2.5cm}|p{2.5cm}|p{1.5cm}|}
\hline
\textbf{No.} & \textbf{Task name} & \textbf{Start date} & \textbf{End date} & \textbf{Duration} \\ \hline
1 & Literature Review \& Project Setup & 29/11/2025 & 01/12/2025 & 3 days \\ \hline
2 & Data Loader Implementation & 29/11/2025 & 01/12/2025 & 3 days \\ \hline
3 & CPU Baseline Implementation & 02/12/2025 & 04/12/2025 & 3 days \\ \hline
4 & GPU Basic Implementation & 05/12/2025 & 07/12/2025 & 3 days \\ \hline
5 & GPU Optimization V1 (Shared Memory) & 08/12/2025 & 10/12/2025 & 3 days \\ \hline
6 & GPU Optimization V2 (Fusion + Streams) & 08/12/2025 & 10/12/2025 & 3 days \\ \hline
7 & Feature Extraction & 11/12/2025 & 13/12/2025 & 3 days \\ \hline
8 & SVM Training Pipeline & 13/12/2025 & 15/12/2025 & 3 days \\ \hline
9 & Demo Interface & 14/12/2025 & 16/12/2025 & 3 days \\ \hline
10 & Performance Analysis \& Documentation & 29/11/2025 & 17/12/2025 & 19 days \\ \hline
11 & Video demo & 16/12/2025 & 18/12/2025 & 3 days \\ \hline
\end{tabular}
\end{center}

\textit{Note: Task 10 (Documentation) is conducted continuously throughout the project. Tasks 8 and 9 can overlap partially.}

\newpage
\subsection*{Work Distribution Summary}
\begin{center}
\begin{tabular}{|c|p{4cm}|c|p{6cm}|}
\hline
\textbf{Student} & \textbf{Tasks assigned} & \textbf{Total tasks} & \textbf{Main responsibilities} \\ \hline
Huỳnh Lê Hải Dương & 1, 2, 3, 4, 9, 10, 11 & 8 & CPU/GPU implementation, optimizations, demo interface, documentation, video demo \\ \hline
Nguyễn Triều Khoáng & 1, 2, 5, 6, 7, 8, 9, 10, 11 & 9 & Literature review, data loader, GPU optimization V2, feature extraction, SVM training, demo interface, documentation, video demo \\ \hline
\end{tabular}
\end{center}

\subsection*{Key Achievements}
\begin{itemize}
    \item \textbf{Performance Optimization}: Achieved 10.91× speedup over CPU baseline through GPU parallelization and optimizations
    \item \textbf{Training Time}: Reduced autoencoder training time from ~5.7 hours (CPU) to ~31.5 minutes (GPU optimized)
    \item \textbf{Feature Extraction}: Successfully extracted 8,192-dimensional features from 60,000 CIFAR-10 images in under 23 seconds
    \item \textbf{Classification Accuracy}: Achieved 67.53\% test accuracy using SVM classifier on extracted features
    \item \textbf{Code Quality}: Implemented modular, well-documented codebase with separate CPU and GPU implementations
    \item \textbf{Complete Pipeline}: Built end-to-end system from data loading to real-time inference via Gradio interface
\end{itemize}

\subsection*{Technologies and Tools Used}
\begin{itemize}
    \item \textbf{Programming Languages}: C++17, CUDA, Python 3
    \item \textbf{GPU Computing}: NVIDIA CUDA Toolkit, cuML (RAPIDS)
    \item \textbf{Deep Learning}: Custom autoencoder implementation (no frameworks)
    \item \textbf{Machine Learning}: LIBSVM, cuML SVM for classification
    \item \textbf{Data Processing}: NumPy, custom CIFAR-10 binary loader
    \item \textbf{Visualization}: Matplotlib, Seaborn, Jupyter Notebook
    \item \textbf{Web Interface}: Gradio for interactive demo
    \item \textbf{Build System}: Makefile for compilation and project management
    \item \textbf{Version Control}: Git for collaborative development
\end{itemize}

\subsection*{Challenges and Solutions}
\begin{itemize}
    \item \textbf{Challenge 1: Upsampling Backward Pass Gradient Accumulation}
        \begin{itemize}
            \item \textit{Problem}: Upsampling forward pass maps one input pixel to four output pixels (2×2 upsampling). During backward pass, four gradient values must be accumulated into a single input gradient position, requiring careful indexing and accumulation logic.
            \item \textit{Solution}: Implemented efficient gradient accumulation by mapping each output gradient back to the corresponding input position using integer division (ih = oh >> 1, iw = ow >> 1) and accumulating gradients correctly without atomic operations.
            \item \textit{Lesson}: When implementing custom layers, carefully track how forward and backward passes map between tensor dimensions. Visualize the data flow at each step to avoid gradient accumulation errors.
        \end{itemize}
    \item \textbf{Challenge 2: Shared Memory Overhead for Small Images}
        \begin{itemize}
            \item \textit{Problem}: Initial attempt to use shared memory tiling for convolution showed that for small images (32×32), the overhead of loading tiles and synchronization exceeded the benefit of data reuse.
            \item \textit{Solution}: Switched to register blocking and weight caching strategy. Used \texttt{\_\_restrict\_\_} pointers and loop unrolling to maximize register usage and compiler optimizations. For small kernels (3×3), this approach proved more efficient than shared memory tiling.
            \item \textit{Lesson}: Not all optimization techniques work for all problem sizes. Profile and measure - what works for large images may not work for small ones. The optimal strategy depends on image size, kernel size, and available shared memory.
        \end{itemize}
    \item \textbf{Challenge 3: Memory Layout and Coalescing}
        \begin{itemize}
            \item \textit{Problem}: Initial naive implementation had poor memory access patterns, with threads accessing non-consecutive memory locations, leading to uncoalesced memory accesses and poor bandwidth utilization (potentially 10× reduction in effective bandwidth).
            \item \textit{Solution}: Reorganized data access patterns to ensure threads in a warp access consecutive memory locations. Used proper indexing schemes and vectorized loads (float4) where possible to maximize memory bandwidth. Ensured memory alignment for vectorized operations.
            \item \textit{Lesson}: Memory access patterns are often more important than raw computation. Uncoalesced accesses can reduce effective bandwidth dramatically. Always design kernels with memory coalescing in mind from the start.
        \end{itemize}
\end{itemize}

\subsection*{Project Statistics}
\begin{center}
\begin{tabular}{|l|r|}
\hline
\textbf{Metric} & \textbf{Value} \\ \hline
Total lines of code (C++/CUDA) & ~5,488 \\ \hline
Total lines of code (Python) & ~680 \\ \hline
Number of CUDA kernels & 15+ \\ \hline
Number of CPU functions & 20+ \\ \hline
Model parameters & 751,875 \\ \hline
Feature dimension & 8,192 \\ \hline
Training images processed & 50,000 \\ \hline
Test images processed & 10,000 \\ \hline
GPU memory usage (batch size 64) & ~814 MB \\ \hline
\end{tabular}
\end{center}


\end{document}
